apiVersion: image.openshift.io/v1
kind: ImageStream
metadata:
  labels:
    app: {{.Values.specialresource.metadata.name}}
  name: {{.Values.specialresource.metadata.name}}
spec: {}
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{.Values.specialresource.metadata.name}}-buildah-helper
data:
  buildah-helper.sh: |-
    #!/bin/bash -x

    CNT=
    MNT=

    MOUNT_MACHINE_OS_CONTENT() { export MOC=$(buildah --authfile /var/lib/kubelet/config.json  --storage-driver vfs from {{.Values.osImageURL}}); export MOCMNT=$(buildah --storage-driver vfs mount $MOC); }
    UMOUNT_MACHINE_OS_CONTENT() { buildah --storage-driver vfs umount $MOC;  }


    FROM() { export CNT=$(buildah --storage-driver vfs from $1); }

    MOUNT() { export MNT=$(buildah --storage-driver vfs mount $CNT); }
    UMOUNT() { buildah --storage-driver vfs umount $CNT; }

    ENV() { buildah --storage-driver vfs config --env $@ $CNT; }
    RUN() { buildah --storage-driver vfs --isolation chroot run --user 0 $CNT -- $@; }
    RUNV() { buildah --storage-driver vfs --isolation chroot run --volume /etc/pki/entitlement-host:/etc/pki/entitlement:z --volume ${MOCMNT}:/extensions:z --user 0 $CNT -- $@; }
    COPY() { buildah --storage-driver vfs copy $CNT $@; }
    COMMIT() { buildah --storage-driver vfs commit $CNT $1; }
    ENTRYPOINT() { buildah config --entrypoint $1 $CNT; }
    WORKINGDIR() { buildah --storage-driver vfs config --workingdir $1 $CNT; }
    PUSH() { buildah --storage-driver vfs push --tls-verify=false --authfile /root/.dockercfg  $@; }
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{.Values.specialresource.metadata.name}}-install-dependencies
data:
  install-dependencies.sh: |-
    #!/bin/bash -x

    set -e

    kernel_version=${KERNEL_VERSION}

    # Adding the full name here so we can use one function
    # to install from EUS, base and machine-os-content (MOC)
    kernel_rpms=(
      kernel-core-${kernel_version}
      kernel-devel-${kernel_version}
      kernel-headers-${kernel_version}
      kernel-modules-${kernel_version}
      kernel-modules-extra-${kernel_version}
    )

    function enable_repo() {
      yum config-manager --set-enabled "${1}" || true
      if ! dnf makecache; then
        yum config-manager --set-disabled "${1}"
        echo "${1} not enabled"
      fi
      echo "${1} enabled"
    }

    function install_kernel_rpms() {
      for rpm in "$@"
      do
        echo "Installing ${rpm}"
        if ! yum -y --best install ${rpm}; then
          echo "Cannot install ${rpm}, aborting EUS installation"
          return 1
        fi
      done
    }

    function install_kernel_rpms_nodeps() {
      for rpm in "$@"
      do
        echo "Installing ${rpm}"
        if ! rpm -ivh --nodeps ${rpm}; then
          echo "Cannot install ${rpm} with rpm -i --nodeps"
          return 1
        fi
      done
    }

    echo "Setting the correct releasever for all following yum comamnds..."
    echo ${RHEL_VERSION} > /etc/yum/vars/releasever

    echo "Setting install_weak_deps=False globally for yum..."
    yum config-manager --setopt=install_weak_deps=False --save

    echo "Enabling RHOCP and EUS RPM repos..."
    enable_repo rhocp-${CLUSTER_VERSION}-for-rhel-8-x86_64-rpms
    enable_repo rhel-8-for-x86_64-baseos-eus-rpms || true
    # PLACEHOLDER FOR RT KERNEL -- enable_repo rhel-8-for-x86_64-nfv-rpms

    # First update the base container to latest versions of everything
    yum update -y

    # Additional packages that are mandatory for driver-containers
    yum -y --best install elfutils-libelf-devel kmod binutils kabi-dw kernel-abi-whitelists

    # Try to enable EUS and try to install kernel-devel and kernel-headers RPMs
    if install_kernel_rpms "${kernel_rpms[@]}"; then
      echo "EUS - kernel rpms ${kernel_rpms[@]} installed"
      exit 0
    fi

    # If EUS fails get kernel-devel and kernel-headers from machine-os-content
    echo "EUS and/or rhocp-${CLUSTER_VERSION} FAILED - installing from machine-os-content"

    # Installation order is important leave this as is
    kernel_rpms=(
      $(find /extensions -name kernel-core-${kernel_version}.rpm -exec ls {} \; | tail -n1)
      $(find /extensions -name kernel-devel-${kernel_version}.rpm -exec ls {} \; | tail -n1)
      $(find /extensions -name kernel-headers-${kernel_version}.rpm -exec ls {} \; | tail -n1)
      $(find /extensions -name kernel-modules-${kernel_version}.rpm -exec ls {} \; | tail -n1)
      $(find /extensions -name kernel-modules-extra-${kernel_version}.rpm -exec ls {} \; | tail -n1)
    )


    # On a 4.5 cluster we only have a subset of these available
    # If they are empty yum will fail anyway, so I do not see the purpose of checking ! -z ...
    # [ ! -z $KERNEL_DEVEL ]
    # [ ! -z $KERNEL_HEADERS ]
    # [ ! -z $KERNEL_CORE ]
    # [ ! -z $KERNEL_MODULES ]
    # [ ! -z $KERNEL_MODULES_EXTRA ]

    if install_kernel_rpms_nodeps "${kernel_rpms[@]}"; then
      echo "MOC - kernel rpms ${kernel_rpms[@]} installed"
      exit 0
    fi

    # Install realtime kernel TODO
    ls /extensions/kernel-rt*

    dnf clean all


---
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{.Values.specialresource.metadata.name}}-entrypoint
data:
  entrypoint.sh: |-
    #!/bin/bash -x
    set -e
    cd /tmp

    yum -y install buildah git make --setopt=install_weak_deps=False

    git clone {{.Values.specialresource.spec.driverContainer.source.git.uri}}

    source /bin/buildah-helper.sh

    SCRIPT_NAME=install-dependencies.sh
    TAG={{.Values.specialresource.spec.namespace}}/{{.Values.specialresource.metadata.name}}:v${KERNEL_VERSION}

    # --------- Container instructions START ----------------------------------

    MOUNT_MACHINE_OS_CONTENT

    FROM registry.access.redhat.com/ubi8/ubi

    WORKINGDIR /tmp

    ENV KERNEL_VERSION=${KERNEL_VERSION}
    ENV RHEL_VERSION=${RHEL_VERSION}
    ENV CLUSTER_VERSION=${CLUSTER_VERSION}

    COPY /bin/${SCRIPT_NAME} .
    RUNV bash -c $(pwd)/${SCRIPT_NAME}

    # Install directly into the chroot, this way we do not have to install
    # additinoal packages like git into the container to install from a git repo
    # The deps are resolved by the outer image.
    MOUNT
    cd kmods-via-containers
    make install DESTDIR=${MNT}/usr/local CONFDIR=${MNT}/etc/
    UMOUNT

    COMMIT ${TAG}
    PUSH   ${TAG} image-registry.openshift-image-registry.svc:5000/${TAG}

    UMOUNT_MACHINE_OS_CONTENT

    # --------- Container instructions END ------------------------------------

    # startupprobe readonlyfilesystem would prevent writing to /
    touch /tmp/ready
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: {{.Values.specialresource.metadata.name}}-{{.Values.groupName.driverContainer}}
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: {{.Values.specialresource.metadata.name}}-{{.Values.groupName.driverContainer}}
rules:
- apiGroups:
  - security.openshift.io
  resources:
  - securitycontextconstraints
  verbs:
  - use
  resourceNames:
  - hostaccess
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: {{.Values.specialresource.metadata.name}}-{{.Values.groupName.driverContainer}}
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: {{.Values.specialresource.metadata.name}}-{{.Values.groupName.driverContainer}}
subjects:
- kind: ServiceAccount
  name: {{.Values.specialresource.metadata.name}}-{{.Values.groupName.driverContainer}}
userNames:
- system:serviceaccount:{{.Values.specialresource.spec.namespace}}:{{.Values.specialresource.metadata.name}}
---
apiVersion: v1
kind: Pod
metadata:
  annotations:
    specialresource.openshift.io/proxy: "true"
    specialresource.openshift.io/wait: "true"
    specialresource.openshift.io/wait-for-logs: "touch /tmp/ready"
    specialresource.openshift.io/kernel-affine: "true"
  labels:
    app: {{.Values.specialresource.metadata.name}}
  name: {{.Values.specialresource.metadata.name}}
spec:
  serviceAccount: {{.Values.specialresource.metadata.name}}-{{.Values.groupName.driverContainer}}
  serviceAccountName: {{.Values.specialresource.metadata.name}}-{{.Values.groupName.driverContainer}}
  restartPolicy: OnFailure
  containers:
  - name: {{.Values.specialresource.metadata.name}}
    image: registry.access.redhat.com/ubi8:latest
    env:
    - name: KERNEL_VERSION
      value: "{{.Values.kernelFullVersion}}"
    - name: RHEL_VERSION
      value: "{{.Values.operatingSystemDecimal}}"
    - name: CLUSTER_VERSION
      value: "{{.Values.clusterVersionMajorMinor}}"

    command: [/bin/entrypoint.sh]
    securityContext:
      seLinuxOptions:
        type: spc_t
    volumeMounts:
        - name: entrypoint
          mountPath: /bin/entrypoint.sh
          readOnly: true
          subPath: entrypoint.sh
        - name: install-dependencies
          mountPath: /bin/install-dependencies.sh
          readOnly: true
          subPath: install-dependencies.sh
        - name: buildah-helper
          mountPath: /bin/buildah-helper.sh
          readOnly: true
          subPath: buildah-helper.sh
        - name: authfile
          mountPath: /var/lib/kubelet
          readOnly: true
        - name: dockercfg
          mountPath: /root
          readOnly: true
    startupProbe:
      exec:
        command: [sh, -c, 'ls /tmp/ready']
      initialDelaySeconds: 10
      failureThreshold: 120
      successThreshold: 1
      periodSeconds: 10
  volumes:
    - name: dockercfg
      secret:
        secretName: {{.Values.pushSecretName}}
    - name: authfile
      hostPath:
        path: /var/lib/kubelet
    - name: entrypoint
      configMap:
        defaultMode: 0700
        name: {{.Values.specialresource.metadata.name}}-entrypoint
    - name: install-dependencies
      configMap:
        defaultMode: 0700
        name: {{.Values.specialresource.metadata.name}}-install-dependencies
    - name: buildah-helper
      configMap:
        defaultMode: 0700
        name: {{.Values.specialresource.metadata.name}}-buildah-helper
