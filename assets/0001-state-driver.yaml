apiVersion: v1
kind: ServiceAccount
metadata:
  name: nvidia-driver
  namespace: openshift-sro
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: nvidia-driver
  namespace: openshift-sro
rules:
- apiGroups:
  - security.openshift.io
  resources:
  - securitycontextconstraints
  verbs:
  - use
  resourceNames:
  - privileged
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: nvidia-driver
  namespace: openshift-sro
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: nvidia-driver
  namespace: openshift-sro
subjects:
- kind: ServiceAccount
  name: nvidia-driver
  namespace: openshift-sro
userNames:
- system:serviceaccount:openshift-sro:nvidia-driver
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: nvidia-driver
  namespace: openshift-sro
data:
  oci-nvidia-hook-json: | 
    {
        "version": "1.0.0",
        "hook": {
            "path": "/run/nvidia/driver/usr/bin/nvidia-container-toolkit",
            "args": ["nvidia-container-runtime-hook", "prestart"],
            "env": [
                "PATH=/run/nvidia/driver/usr/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"
            ]
        },
        "when": {
            "always": true,
            "commands": [".*"]
        },
        "stages": ["prestart"]
    }
---
# Please edit the object below. Lines beginning with a '#' will be ignored,
# and an empty file will abort the edit. If an error occurs while saving this file will be
# reopened with the relevant failures.
#
allowHostDirVolumePlugin: true
allowHostIPC: false
allowHostNetwork: false
allowHostPID: true
allowHostPorts: false
allowPrivilegeEscalation: true
allowPrivilegedContainer: true
allowedCapabilities:
- '*'
allowedUnsafeSysctls:
- '*'
apiVersion: security.openshift.io/v1
defaultAddCapabilities: null
fsGroup:
  type: RunAsAny
groups:
- system:cluster-admins
- system:nodes
- system:masters
kind: SecurityContextConstraints
metadata:
  annotations:
    kubernetes.io/description: 'privileged allows access to all privileged and host
      features and the ability to run as any user, any group, any fsGroup, and with
      any SELinux context.  WARNING: this is the most relaxed SCC and should be used
      only for cluster administration. Grant with caution.'

  name: nvidia-driver
priority: null
readOnlyRootFilesystem: false
requiredDropCapabilities: null
runAsUser:
  type: RunAsAny
seLinuxContext:
  type: RunAsAny
seccompProfiles:
- '*'
supplementalGroups:
  type: RunAsAny
users:
- system:serviceaccount:openshift-sro:nvidia-driver
volumes:
- '*'
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  labels:
    app: nvidia-driver-daemonset
  name: nvidia-driver-daemonset
  namespace: openshift-sro
  annotations:
    openshift.io/scc: nvidia-driver
    callback: nvidia-driver-daemonset
spec:
  selector:
    matchLabels:
      app: nvidia-driver-daemonset
  template:
    metadata:
      # Mark this pod as a critical add-on; when enabled, the critical add-on scheduler
      # reserves resources for critical add-on pods so that they can be rescheduled after
      # a failure.  This annotation works in tandem with the toleration below.
      annotations:
        scheduler.alpha.kubernetes.io/critical-pod: ""
      labels:
        app: nvidia-driver-daemonset
    spec:
      tolerations:
      - operator: Exists
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule
      serviceAccount: nvidia-driver
      serviceAccountName: nvidia-driver
      hostPID: true
      containers:
      - image: quay.io/openshift-psap/nvidia-driver:v430.34-KERNEL_FULL_VERSION
        imagePullPolicy: Always
        name: nvidia-driver-ctr
        command: ["nvidia-driver"]
        args: ["init"]
        securityContext:
          privileged: true
          seLinuxOptions:
            level: "s0"
        volumeMounts:
          - name: run-nvidia
            mountPath: /run/nvidia
            mountPropagation: Bidirectional
          - name: host-hooks
            mountPath: /host/etc/containers/oci
          - name: config
            mountPath: /etc/containers/oci/hooks.d
          - name: host-modules
            mountPath: /host/lib/modules
      volumes:
        - name: host-hooks
          hostPath:
            path: /etc/containers/oci
        - name: host-modules
          hostPath:
            path: /lib/modules
        - name: run-nvidia
          hostPath:
            path: /run/nvidia
        - name: config
          configMap:
            name: nvidia-driver
            items:
              - key: oci-nvidia-hook-json
                path: oci-nvidia-hook.json
      nodeSelector:
        node-role.kubernetes.io/worker: ""
        feature.node.kubernetes.io/pci-10de.present: "true"
        feature.node.kubernetes.io/kernel-version.full: "KERNEL_FULL_VERSION"
