apiVersion: v1
kind: ServiceAccount
metadata:
  name: nvidia-dcgm-exporter
  namespace: openshift-sro
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: nvidia-dcgm-exporter
  namespace: openshift-sro
rules:
- apiGroups:
  - security.openshift.io
  resources:
  - securitycontextconstraints
  verbs:
  - use
  resourceNames:
  - privileged 
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: nvidia-dcgm-exporter
  namespace: openshift-sro
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: nvidia-dcgm-exporter
subjects:
- kind: ServiceAccount
  name: nvidia-dcgm-exporter
userNames:
- system:serviceaccount:openshift-sro:nvidia-dcgm-exporter
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: prometheus-k8s
  namespace: openshift-sro
rules:
 - apiGroups:
   - ""
   resources:
   - services
   - endpoints
   - pods
   verbs:
   - get
   - list
   - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: prometheus-k8s
  namespace: openshift-sro
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: prometheus-k8s
subjects:
- kind: ServiceAccount
  name: prometheus-k8s
  namespace: openshift-monitoring
---
apiVersion: v1
kind: Service
metadata:
  namespace: openshift-sro
  name: nvidia-dcgm-exporter
  labels:
    app: nvidia-dcgm-exporter
spec:
  selector:
    app: nvidia-dcgm-exporter
  type: ClusterIP
  ports:
  - name: dcgm
    port: 9600
    targetPort: 9600
    protocol: TCP
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  namespace: openshift-sro
  name: nvidia-dcgm-exporter
spec:
  endpoints:
  - port: dcgm 
  jobLabel: app
  namespaceSelector:
    matchNames:
    - openshift-sro
  selector:
    matchLabels:
      app: nvidia-dcgm-exporter
---
# Node exporter collecting only GPU metrics from dcgm-exporter.
# Except textfile collector, all other collectors that are enabled by default are disabled.
# Refer: https://github.com/prometheus/node_exporter/tree/release-0.16
apiVersion: apps/v1
kind: DaemonSet
metadata:
  labels: 
    app: nvidia-dcgm-exporter
  name: nvidia-dcgm-exporter
  namespace: openshift-sro
spec:
  selector:
    matchLabels:
      app: nvidia-dcgm-exporter
  template:
    metadata:
      labels:
        app: nvidia-dcgm-exporter
      name: nvidia-dcgm-exporter
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: specialresource.openshift.io/device-validation
                operator: In 
                values:
                - ready 
      tolerations:
      - operator: Exists
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule
      serviceAccount: nvidia-dcgm-exporter
      serviceAccountName: nvidia-dcgm-exporter
      containers:
      - image: quay.io/prometheus/node-exporter:v0.16.0
        name: node-exporter
        args:
        - "--web.listen-address=0.0.0.0:9600"
        - "--path.procfs=/host/proc"
        - "--path.sysfs=/host/sys"
        - "--collector.textfile.directory=/run/prometheus"
        - "--no-collector.arp"
        - "--no-collector.bcache"
        - "--no-collector.bonding"
        - "--no-collector.conntrack"
        - "--no-collector.cpu"
        - "--no-collector.diskstats"
        - "--no-collector.edac"
        - "--no-collector.entropy"
        - "--no-collector.filefd"
        - "--no-collector.filesystem"
        - "--no-collector.hwmon"
        - "--no-collector.infiniband"
        - "--no-collector.ipvs"
        - "--no-collector.loadavg"
        - "--no-collector.mdadm"
        - "--no-collector.meminfo"
        - "--no-collector.netdev"
        - "--no-collector.netstat"
        - "--no-collector.nfs"
        - "--no-collector.nfsd"
        - "--no-collector.sockstat"
        - "--no-collector.stat"
        - "--no-collector.time"
        - "--no-collector.timex"
        - "--no-collector.uname"
        - "--no-collector.vmstat"
        - "--no-collector.wifi"
        - "--no-collector.xfs"
        - "--no-collector.zfs"
        ports:
        - name: metrics
          containerPort: 9600
          hostPort: 9600
        resources:
          requests:
            memory: 30Mi
            cpu: 100m
          limits:
            memory: 50Mi
            cpu: 200m
        volumeMounts:
        - name: proc
          readOnly:  true
          mountPath: /host/proc
        - name: sys
          readOnly: true
          mountPath: /host/sys
        - name: collector-textfiles
          readOnly: true
          mountPath: /run/prometheus
      - image: nvidia/dcgm-exporter:1.4.6
        name: nvidia-dcgm-exporter
        securityContext:
          runAsNonRoot: false
          runAsUser: 0
        volumeMounts:
        - name: collector-textfiles
          mountPath: /run/prometheus

      hostNetwork: true
      hostPID: true

      volumes:
      - name: proc
        hostPath:
          path: /proc
      - name: sys
        hostPath:
          path: /sys
      - name: collector-textfiles
        emptyDir:
          medium: Memory
      nodeSelector:
        node-role.kubernetes.io/worker: ""
        feature.node.kubernetes.io/pci-10de.present: "true"
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: example
    role: alert-rules
  name: prometheus-example-rules
spec:
  groups:
  - name: nvidia_gpu.rules
    rules:
    - alert: GPUUnusedResources
      expr: dcgm_gpu_utilization == 0
      for: 10m 
      labels:
        severity: warning
      annotations:
        description: GPU idle for the last 5min, do some work!
        summary: GPU Node sad, nothing to do
    - alert: GPUTemperatureTooHigh
      expr: dcgm_gpu_temp > 60
      for: 10m
      labels:
        severity: warning
      annotations:
        description: GPU temperature too high for the last 10min
        summary: GPU Node sad, it is getting hot in here
    - alert: GPUTemperatureTooHigh
      expr: dcgm_gpu_temp > 70
      for: 10m
      labels:
        severity: warning
      annotations:
        description: GPU temperature very high for the last 10min
        summary: GPU Node sad, it is getting hot in here
    - alert: GPUTemperatureTooHigh
      expr: dcgm_gpu_temp > 80
      for: 10m
      labels:
        severity: critical
      annotations:
        description: GPU temperature critical for the last 10min
        summary: GPU Node sad, it is getting hot in here
    - alert: GPUThermalViolation
      expr: dcgm_thermal_violation > 1
      for: 1m
      labels:
        severity: critical
      annotations:
        description: GPU Thermal Violation
        summary: GPU Node is too hot, may break your GPU
