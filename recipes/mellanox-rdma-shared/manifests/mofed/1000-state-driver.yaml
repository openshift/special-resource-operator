apiVersion: v1
kind: ServiceAccount
metadata:
  name: {{.HardwareResource}}-{{.GroupName.DriverContainer}}-{{.OperatingSystemMajor}}
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: {{.HardwareResource}}-{{.GroupName.DriverContainer}}-{{.OperatingSystemMajor}}
rules:
- apiGroups:
  - security.openshift.io
  resources:
  - securitycontextconstraints
  verbs:
  - use
  resourceNames:
  - privileged
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: {{.HardwareResource}}-{{.GroupName.DriverContainer}}-{{.OperatingSystemMajor}}
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: {{.HardwareResource}}-{{.GroupName.DriverContainer}}-{{.OperatingSystemMajor}}
subjects:
- kind: ServiceAccount
  name: {{.HardwareResource}}-{{.GroupName.DriverContainer}}-{{.OperatingSystemMajor}}
userNames:
- system:serviceaccount:{{.OperatorNamespace}}:{{.HardwareResource}}-{{.GroupName.DriverContainer}}
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{.HardwareResource}}-{{.GroupName.DriverContainer}}-{{.OperatingSystemMajor}}-entrypoint
data:
  entrypoint.sh: |-
    #!/bin/bash -x

    mount_rootfs() {
      echo "Mounting {{.HardwareResource}} driver rootfs..."
      mount --make-runbindable /sys
      mount --make-private /sys
      mkdir -p /run/mofed/driver
      mount --rbind / /run/mofed/driver
    }

    unmount_rootfs() {
      echo "Unmounting {{.HardwareResource}} driver rootfs..."
      if findmnt -r -o TARGET | grep "/run/mofed/driver" > /dev/null; then
        umount -l -R /run/mofed/driver
      fi
    }

    mount_rootfs

    trap "echo 'Caught signal'; exit 1" HUP INT QUIT PIPE TERM
    trap "unmount_rootfs" EXIT
        
    sleep infinity
---
# Please edit the object below. Lines beginning with a '#' will be ignored,
# and an empty file will abort the edit. If an error occurs while saving this file will be
# reopened with the relevant failures.
#
allowHostDirVolumePlugin: true
allowHostIPC: false
allowHostNetwork: false
allowHostPID: true
allowHostPorts: false
allowPrivilegeEscalation: true
allowPrivilegedContainer: true
allowedCapabilities:
- '*'
allowedUnsafeSysctls:
- '*'
apiVersion: security.openshift.io/v1
defaultAddCapabilities: null
fsGroup:
  type: RunAsAny
groups:
- system:cluster-admins
- system:nodes
- system:masters
kind: SecurityContextConstraints
metadata:
  annotations:
    kubernetes.io/description: 'privileged allows access to all privileged and host
      features and the ability to run as any user, any group, any fsGroup, and with
      any SELinux context.  WARNING: this is the most relaxed SCC and should be used
      only for cluster administration. Grant with caution.'

  name: {{.HardwareResource}}-{{.GroupName.DriverContainer}}-{{.OperatingSystemMajor}}
priority: null
readOnlyRootFilesystem: false
requiredDropCapabilities: null
runAsUser:
  type: RunAsAny
seLinuxContext:
  type: RunAsAny
seccompProfiles:
- '*'
supplementalGroups:
  type: RunAsAny
users:
- system:serviceaccount:{{.OperatorNamespace}}:{{.HardwareResource}}-{{.GroupName.DriverContainer}}-{{.OperatingSystemMajor}}
volumes:
- '*'
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  labels:
    app: {{.HardwareResource}}-{{.GroupName.DriverContainer}}-{{.OperatingSystemMajor}}
  name: {{.HardwareResource}}-{{.GroupName.DriverContainer}}-{{.OperatingSystemMajor}}
  annotations:
    openshift.io/scc: {{.HardwareResource}}-{{.GroupName.DriverContainer}}-{{.OperatingSystemMajor}}
    specialresource.openshift.io/wait: "true"
    specialresrouce.openshift.io/wait-for-logs: "\\+ sleep infinity"
    specialresource.openshift.io/state: "driver-container"
    specialresource.openshift.io/driver-container-vendor: {{.NodeFeature}}    
spec:
  selector:
    matchLabels:
      app: {{.HardwareResource}}-{{.GroupName.DriverContainer}}-{{.OperatingSystemMajor}}
  template:
    metadata:
      # Mark this pod as a critical add-on; when enabled, the critical add-on scheduler
      # reserves resources for critical add-on pods so that they can be rescheduled after
      # a failure.  This annotation works in tandem with the toleration below.
      annotations:
        scheduler.alpha.kubernetes.io/critical-pod: ""
      labels:
        app: {{.HardwareResource}}-{{.GroupName.DriverContainer}}-{{.OperatingSystemMajor}}
    spec:
      tolerations:
      - operator: Exists
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule
      serviceAccount: {{.HardwareResource}}-{{.GroupName.DriverContainer}}-{{.OperatingSystemMajor}}
      serviceAccountName: {{.HardwareResource}}-{{.GroupName.DriverContainer}}-{{.OperatingSystemMajor}}
      hostPID: true
      containers:
      - image: image-registry.openshift-image-registry.svc:5000/{{.OperatorNamespace}}/{{.HardwareResource}}-{{.GroupName.DriverContainer}}:v{{.KernelVersion}}
        imagePullPolicy: Always
        name: {{.HardwareResource}}-{{.GroupName.DriverContainer}}-{{.OperatingSystemMajor}}
        command: ["/bin/entrypoint.sh"]
        securityContext:
          privileged: true
          seLinuxOptions:
            level: "s0"
        volumeMounts:
          - name: run-{{.HardwareResource}}
            mountPath: /run/mofed
            mountPropagation: Bidirectional
          - name: entrypoint
            mountPath: /bin/entrypoint.sh
            readOnly: true
            subPath: entrypoint.sh
      volumes:
        - name: run-{{.HardwareResource}}
          hostPath:
            path: /run/mofed
        - name: entrypoint
          configMap:
            defaultMode: 0700
            name: {{.HardwareResource}}-{{.GroupName.DriverContainer}}-{{.OperatingSystemMajor}}-entrypoint
      nodeSelector:
        node-role.kubernetes.io/worker: ""
        {{.NodeFeature}}: "true"
        feature.node.kubernetes.io/kernel-version.full: "{{.KernelVersion}}"
