apiVersion: v1
kind: ServiceAccount
metadata:
  name: {{.HardwareResource}}-{{.GroupName.DriverContainer}}-{{.OperatingSystemMajor}}
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: {{.HardwareResource}}-{{.GroupName.DriverContainer}}-{{.OperatingSystemMajor}}
rules:
- apiGroups:
  - security.openshift.io
  resources:
  - securitycontextconstraints
  verbs:
  - use
  resourceNames:
  - privileged
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: {{.HardwareResource}}-{{.GroupName.DriverContainer}}-{{.OperatingSystemMajor}}
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: {{.HardwareResource}}-{{.GroupName.DriverContainer}}-{{.OperatingSystemMajor}}
subjects:
- kind: ServiceAccount
  name: {{.HardwareResource}}-{{.GroupName.DriverContainer}}-{{.OperatingSystemMajor}}
userNames:
- system:serviceaccount:{{.OperatorNamespace}}:{{.HardwareResource}}-{{.GroupName.DriverContainer}}
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{.HardwareResource}}-{{.GroupName.DriverContainer}}-{{.OperatingSystemMajor}}-entrypoint
data:
  entrypoint.sh: |-
    #!/bin/bash -x

    MOFED=/run/mofed/driver
    NVIDIA=/run/nvidia/driver
    KERNEL_VERSION=$(uname -r)

    ln -sf ${MOFED}/usr/src/ofa_kernel /usr/src/ofa_kernel
    ln -sf ${NVIDIA}/usr/src/nvidia-* /usr/src/.

    mkdir -p /lib/modules/${KERNEL_VERSION}
    ln -sf /usr/src/kernels/${KERNEL_VERSION} /lib/modules/${KERNEL_VERSION}/build
    touch /lib/modules/${KERNEL_VERSION}/modules.order
    touch /lib/modules/${KERNEL_VERSION}/modules.builtin

    ln -sf ${NVIDIA}/lib/modules/4.18.0-147.5.1.el8_1.x86_64/kernel /lib/modules/4.18.0-147.5.1.el8_1.x86_64/.

    cd /root 

    dnf -y group install "Development Tools"
    dnf -y install kernel-devel-${KERNEL_VERSION} kernel-headers-${KERNEL_VERSION} kmod binutils perl elfutils-libelf-devel
    git clone https://github.com/Mellanox/nv_peer_memory.git

    cd /root/nv_peer_memory
    sed -i 's/updates\/dkms/kernel\/drivers\/video/g' create_nv.symvers.sh 
    ./build_module.sh

    rpmbuild --rebuild /tmp/nvidia_peer_memory-*

    #COPY --from=BUILDER /root/rpmbuild/RPMS/x86_64/nvidia_peer_memory-*.rpm  .
    dnf -y --setopt=install_weak_deps=False --best install kmod binutils perl
    # rpm  -ivh /root/rpmbuild/RPMS/x86_64/nvidia_peer_memory-*.rpm 

    sleep infinity
---
# Please edit the object below. Lines beginning with a '#' will be ignored,
# and an empty file will abort the edit. If an error occurs while saving this file will be
# reopened with the relevant failures.
#
allowHostDirVolumePlugin: true
allowHostIPC: false
allowHostNetwork: false
allowHostPID: true
allowHostPorts: false
allowPrivilegeEscalation: true
allowPrivilegedContainer: true
allowedCapabilities:
- '*'
allowedUnsafeSysctls:
- '*'
apiVersion: security.openshift.io/v1
defaultAddCapabilities: null
fsGroup:
  type: RunAsAny
groups:
- system:cluster-admins
- system:nodes
- system:masters
kind: SecurityContextConstraints
metadata:
  annotations:
    kubernetes.io/description: 'privileged allows access to all privileged and host
      features and the ability to run as any user, any group, any fsGroup, and with
      any SELinux context.  WARNING: this is the most relaxed SCC and should be used
      only for cluster administration. Grant with caution.'

  name: {{.HardwareResource}}-{{.GroupName.DriverContainer}}-{{.OperatingSystemMajor}}
priority: null
readOnlyRootFilesystem: false
requiredDropCapabilities: null
runAsUser:
  type: RunAsAny
seLinuxContext:
  type: RunAsAny
seccompProfiles:
- '*'
supplementalGroups:
  type: RunAsAny
users:
- system:serviceaccount:{{.OperatorNamespace}}:{{.HardwareResource}}-{{.GroupName.DriverContainer}}-{{.OperatingSystemMajor}}
volumes:
- '*'
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  labels:
    app: {{.HardwareResource}}-{{.GroupName.DriverContainer}}-{{.OperatingSystemMajor}}
  name: {{.HardwareResource}}-{{.GroupName.DriverContainer}}-{{.OperatingSystemMajor}}
  annotations:
    openshift.io/scc: {{.HardwareResource}}-{{.GroupName.DriverContainer}}-{{.OperatingSystemMajor}}
    specialresource.openshift.io/wait: "true"
    specialresrouce.openshift.io/wait-for-logs: "\\+ sleep infinity"
    specialresource.openshift.io/state: "driver-container"
    specialresource.openshift.io/driver-container-vendor: {{.NodeFeature}}    
spec:
  selector:
    matchLabels:
      app: {{.HardwareResource}}-{{.GroupName.DriverContainer}}-{{.OperatingSystemMajor}}
  template:
    metadata:
      # Mark this pod as a critical add-on; when enabled, the critical add-on scheduler
      # reserves resources for critical add-on pods so that they can be rescheduled after
      # a failure.  This annotation works in tandem with the toleration below.
      annotations:
        scheduler.alpha.kubernetes.io/critical-pod: ""
      labels:
        app: {{.HardwareResource}}-{{.GroupName.DriverContainer}}-{{.OperatingSystemMajor}}
    spec:
      tolerations:
      - operator: Exists
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule
      serviceAccount: {{.HardwareResource}}-{{.GroupName.DriverContainer}}-{{.OperatingSystemMajor}}
      serviceAccountName: {{.HardwareResource}}-{{.GroupName.DriverContainer}}-{{.OperatingSystemMajor}}
      hostPID: true
      containers:
      - image: registry.redhat.io/ubi8/ubi:latest
        imagePullPolicy: Always
        name: {{.HardwareResource}}-{{.GroupName.DriverContainer}}-{{.OperatingSystemMajor}}
        command: ["/bin/entrypoint.sh"]
        securityContext:
          privileged: true
          seLinuxOptions:
            level: "s0"
        volumeMounts:
          - name: run-{{.HardwareResource}}-mofed
            mountPath: /run/mofed
          - name: run-{{.HardwareResource}}-nvidia
            mountPath: /run/nvidia
          - name: entrypoint
            mountPath: /bin/entrypoint.sh
            readOnly: true
            subPath: entrypoint.sh
      volumes:
        - name: run-{{.HardwareResource}}-mofed
          hostPath:
            path: /run/mofed
        - name: run-{{.HardwareResource}}-nvidia
          hostPath:
            path: /run/nvidia
        - name: entrypoint
          configMap:
            defaultMode: 0700
            name: {{.HardwareResource}}-{{.GroupName.DriverContainer}}-{{.OperatingSystemMajor}}-entrypoint
      nodeSelector:
        node-role.kubernetes.io/worker: ""
        {{.NodeFeature}}: "true"
        feature.node.kubernetes.io/kernel-version.full: "{{.KernelVersion}}"
